[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "aiohttp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "aiohttp",
        "description": "aiohttp",
        "detail": "aiohttp",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "aiofiles",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "aiofiles",
        "description": "aiofiles",
        "detail": "aiofiles",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.asyncio",
        "description": "tqdm.asyncio",
        "isExtraImport": true,
        "detail": "tqdm.asyncio",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "html",
        "importPath": "lxml",
        "description": "lxml",
        "isExtraImport": true,
        "detail": "lxml",
        "documentation": {}
    },
    {
        "label": "async_playwright",
        "importPath": "playwright.async_api",
        "description": "playwright.async_api",
        "isExtraImport": true,
        "detail": "playwright.async_api",
        "documentation": {}
    },
    {
        "label": "Route",
        "importPath": "playwright.async_api",
        "description": "playwright.async_api",
        "isExtraImport": true,
        "detail": "playwright.async_api",
        "documentation": {}
    },
    {
        "label": "TimeoutError",
        "importPath": "playwright.async_api",
        "description": "playwright.async_api",
        "isExtraImport": true,
        "detail": "playwright.async_api",
        "documentation": {}
    },
    {
        "label": "async_playwright",
        "importPath": "playwright.async_api",
        "description": "playwright.async_api",
        "isExtraImport": true,
        "detail": "playwright.async_api",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "sync_playwright",
        "importPath": "playwright.sync_api",
        "description": "playwright.sync_api",
        "isExtraImport": true,
        "detail": "playwright.sync_api",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "webdriver",
        "importPath": "selenium",
        "description": "selenium",
        "isExtraImport": true,
        "detail": "selenium",
        "documentation": {}
    },
    {
        "label": "By",
        "importPath": "selenium.webdriver.common.by",
        "description": "selenium.webdriver.common.by",
        "isExtraImport": true,
        "detail": "selenium.webdriver.common.by",
        "documentation": {}
    },
    {
        "label": "Options",
        "importPath": "selenium.webdriver.chrome.options",
        "description": "selenium.webdriver.chrome.options",
        "isExtraImport": true,
        "detail": "selenium.webdriver.chrome.options",
        "documentation": {}
    },
    {
        "label": "sanitize_filename",
        "kind": 2,
        "importPath": "DramaList_Scrapper.dramaImage",
        "description": "DramaList_Scrapper.dramaImage",
        "peekOfCode": "def sanitize_filename(name):\n    \"\"\"Remove invalid characters for safe file naming.\"\"\"\n    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", str(name)).strip()\nUSER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_2) AppleWebKit/605.1.15 \"\n    \"(KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:126.0) Gecko/20100101 Firefox/126.0\",\n    \"Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 \"",
        "detail": "DramaList_Scrapper.dramaImage",
        "documentation": {}
    },
    {
        "label": "download_images_from_csv",
        "kind": 2,
        "importPath": "DramaList_Scrapper.dramaImage",
        "description": "DramaList_Scrapper.dramaImage",
        "peekOfCode": "def download_images_from_csv(csv_path, output_folder, concurrency=100):\n    \"\"\"Download only missing images from a CSV/Excel file.\"\"\"\n    # Read data\n    if csv_path.lower().endswith(('.xlsx', '.xls')):\n        df = pd.read_excel(csv_path)\n    else:\n        df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n    if \"title\" not in df.columns or \"image\" not in df.columns:\n        raise ValueError(\"The file must contain 'title' and 'image' columns.\")\n    os.makedirs(output_folder, exist_ok=True)",
        "detail": "DramaList_Scrapper.dramaImage",
        "documentation": {}
    },
    {
        "label": "USER_AGENTS",
        "kind": 5,
        "importPath": "DramaList_Scrapper.dramaImage",
        "description": "DramaList_Scrapper.dramaImage",
        "peekOfCode": "USER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_2) AppleWebKit/605.1.15 \"\n    \"(KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:126.0) Gecko/20100101 Firefox/126.0\",\n    \"Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/125.0.0.0 Mobile Safari/537.36\",\n    \"Mozilla/5.0 (iPhone; CPU iPhone OS 17_3 like Mac OS X) \"\n    \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1\",",
        "detail": "DramaList_Scrapper.dramaImage",
        "documentation": {}
    },
    {
        "label": "extract_mydramalist_data",
        "kind": 2,
        "importPath": "DramaList_Scrapper.scrapper",
        "description": "DramaList_Scrapper.scrapper",
        "peekOfCode": "def extract_mydramalist_data(file_path):\n    \"\"\"Ultra-fast extractor using lxml (no BeautifulSoup).\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            text = f.read()\n    except Exception:\n        return None\n    try:\n        doc = html.fromstring(text)\n    except Exception:",
        "detail": "DramaList_Scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "process_folder",
        "kind": 2,
        "importPath": "DramaList_Scrapper.scrapper",
        "description": "DramaList_Scrapper.scrapper",
        "peekOfCode": "def process_folder(input_path, output_csv=\"output_fast.csv\", max_workers=None, skip_existing=True):\n    \"\"\"Process all HTML files using multithreading and lxml for maximum speed.\"\"\"\n    if not os.path.exists(input_path):\n        print(f\"Path not found: {input_path}\")\n        return\n    # Collect HTML files\n    if os.path.isdir(input_path):\n        files = [os.path.join(input_path, f) for f in os.listdir(input_path)\n                 if f.lower().endswith(\".html\")]\n    else:",
        "detail": "DramaList_Scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "TVSERIES_RE",
        "kind": 5,
        "importPath": "DramaList_Scrapper.scrapper",
        "description": "DramaList_Scrapper.scrapper",
        "peekOfCode": "TVSERIES_RE = re.compile('TVSeries')\nDESC_CLASS_RE = re.compile(r'(show-synopsis|show-synopsis__text|show-details-item__content)')\ndef extract_mydramalist_data(file_path):\n    \"\"\"Ultra-fast extractor using lxml (no BeautifulSoup).\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            text = f.read()\n    except Exception:\n        return None\n    try:",
        "detail": "DramaList_Scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "DESC_CLASS_RE",
        "kind": 5,
        "importPath": "DramaList_Scrapper.scrapper",
        "description": "DramaList_Scrapper.scrapper",
        "peekOfCode": "DESC_CLASS_RE = re.compile(r'(show-synopsis|show-synopsis__text|show-details-item__content)')\ndef extract_mydramalist_data(file_path):\n    \"\"\"Ultra-fast extractor using lxml (no BeautifulSoup).\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            text = f.read()\n    except Exception:\n        return None\n    try:\n        doc = html.fromstring(text)",
        "detail": "DramaList_Scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "remove_emojis",
        "kind": 2,
        "importPath": "DramaList_Scrapper.scrapper_2",
        "description": "DramaList_Scrapper.scrapper_2",
        "peekOfCode": "def remove_emojis(text):\n    return re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n# -------------------------------------------\n# Download single page\n# -------------------------------------------\nasync def download_page(i, url, context, output_dir, semaphore):\n    url = url.strip()\n    if not url or url.lower() in [\"n/a\", \"none\", \"null\"] or not url.startswith(\"https://mydramalist.com/\"):\n        print(f\"[{i}] Skipping invalid URL: {url}\")\n        return",
        "detail": "DramaList_Scrapper.scrapper_2",
        "documentation": {}
    },
    {
        "label": "USER_AGENTS",
        "kind": 5,
        "importPath": "DramaList_Scrapper.scrapper_2",
        "description": "DramaList_Scrapper.scrapper_2",
        "peekOfCode": "USER_AGENTS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:127.0) Gecko/20100101 Firefox/127.0\",\n    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\",\n]\n# -------------------------------------------\n# Block unnecessary resources\n# -------------------------------------------\nasync def block_images_and_fonts(route: Route):",
        "detail": "DramaList_Scrapper.scrapper_2",
        "documentation": {}
    },
    {
        "label": "extract_drama_data_from_html",
        "kind": 2,
        "importPath": "html_extractor_and_reader",
        "description": "html_extractor_and_reader",
        "peekOfCode": "def extract_drama_data_from_html(html_content):\n    \"\"\"\n    Extracts drama information from the given HTML content string.\n    Returns a list of dictionaries.\n    \"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n    drama_items = soup.find_all('div', class_='box')\n    extracted_data = []\n    BASE_URL = \"https://mydramalist.com\"\n    for item in drama_items:",
        "detail": "html_extractor_and_reader",
        "documentation": {}
    },
    {
        "label": "extract_from_folder",
        "kind": 2,
        "importPath": "html_extractor_and_reader",
        "description": "html_extractor_and_reader",
        "peekOfCode": "def extract_from_folder(folder_path, output_csv):\n    \"\"\"\n    Loops through all .html files in the given folder,\n    extracts drama data from each file, and saves everything to one CSV.\n    \"\"\"\n    all_data = []\n    for filename in os.listdir(folder_path):\n        if filename.lower().endswith(\".html\"):\n            file_path = os.path.join(folder_path, filename)\n            print(f\"Processing file: {filename}\")",
        "detail": "html_extractor_and_reader",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "kissh_extractor",
        "description": "kissh_extractor",
        "peekOfCode": "URL = \"https://kisskh.co/Explore?status=2&country=2&sub=1&type=1&order=1\"\n# Use a distinct filename for the debug run\nFILENAME = \"kissh_debug_output.html\"\n# Run the asynchronous function\nif __name__ == \"__main__\":\n    asyncio.run(debug_html_fetch_with_playwright(URL, FILENAME))",
        "detail": "kissh_extractor",
        "documentation": {}
    },
    {
        "label": "FILENAME",
        "kind": 5,
        "importPath": "kissh_extractor",
        "description": "kissh_extractor",
        "peekOfCode": "FILENAME = \"kissh_debug_output.html\"\n# Run the asynchronous function\nif __name__ == \"__main__\":\n    asyncio.run(debug_html_fetch_with_playwright(URL, FILENAME))",
        "detail": "kissh_extractor",
        "documentation": {}
    },
    {
        "label": "remove_refs",
        "kind": 2,
        "importPath": "wiki_scrapper_playwright",
        "description": "wiki_scrapper_playwright",
        "peekOfCode": "def remove_refs(text):\n    \"\"\"Remove all reference markers like [1], [ko], [citation needed].\"\"\"\n    return re.sub(r'\\[.*?\\]', '', str(text))\ndef clean_multiline(text):\n    \"\"\"Convert multiline text to properly comma-separated clean string.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\"]:\n        return \"-\"\n    text = remove_refs(text)\n    parts = re.split(r'[\\n\\r]+', str(text))\n    parts = [p.strip() for p in parts if p.strip()]",
        "detail": "wiki_scrapper_playwright",
        "documentation": {}
    },
    {
        "label": "clean_multiline",
        "kind": 2,
        "importPath": "wiki_scrapper_playwright",
        "description": "wiki_scrapper_playwright",
        "peekOfCode": "def clean_multiline(text):\n    \"\"\"Convert multiline text to properly comma-separated clean string.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\"]:\n        return \"-\"\n    text = remove_refs(text)\n    parts = re.split(r'[\\n\\r]+', str(text))\n    parts = [p.strip() for p in parts if p.strip()]\n    return \", \".join(parts) if parts else \"-\"\ndef clean_description(desc):\n    \"\"\"Clean [1], etc. and normalize spaces.\"\"\"",
        "detail": "wiki_scrapper_playwright",
        "documentation": {}
    },
    {
        "label": "clean_description",
        "kind": 2,
        "importPath": "wiki_scrapper_playwright",
        "description": "wiki_scrapper_playwright",
        "peekOfCode": "def clean_description(desc):\n    \"\"\"Clean [1], etc. and normalize spaces.\"\"\"\n    if not desc:\n        return \"-\"\n    desc = remove_refs(desc)\n    desc = re.sub(r'\\s+', ' ', desc).strip()\n    return desc if desc else \"-\"\ndef extract_years_from_release(text):\n    \"\"\"Extract release year(s) from release date text.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\", \"-\"]:",
        "detail": "wiki_scrapper_playwright",
        "documentation": {}
    },
    {
        "label": "extract_years_from_release",
        "kind": 2,
        "importPath": "wiki_scrapper_playwright",
        "description": "wiki_scrapper_playwright",
        "peekOfCode": "def extract_years_from_release(text):\n    \"\"\"Extract release year(s) from release date text.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\", \"-\"]:\n        return \"-\"\n    text = remove_refs(text)\n    full_years = re.findall(r'(?:19|20)\\d{2}', text)\n    unique_years = list(dict.fromkeys(full_years))  # preserve order\n    return \", \".join(unique_years) if unique_years else \"-\"\n# =========================================================\n# Fallback: Description method",
        "detail": "wiki_scrapper_playwright",
        "documentation": {}
    },
    {
        "label": "get_description_fallback",
        "kind": 2,
        "importPath": "wiki_scrapper_playwright",
        "description": "wiki_scrapper_playwright",
        "peekOfCode": "def get_description_fallback(url):\n    headers = {\n        \"User-Agent\": (\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n            \"Chrome/118.0.0.0 Safari/537.36\"\n        )\n    }\n    try:\n        response = requests.get(url, headers=headers, timeout=10)",
        "detail": "wiki_scrapper_playwright",
        "documentation": {}
    },
    {
        "label": "remove_refs",
        "kind": 2,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "def remove_refs(text):\n    \"\"\"Remove all reference markers like [1], [ko], [citation needed].\"\"\"\n    return re.sub(r'\\[.*?\\]', '', str(text))\ndef clean_multiline(text):\n    \"\"\"Convert multiline text to properly comma-separated clean string.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\"]:\n        return \"-\"\n    text = remove_refs(text)\n    parts = re.split(r'[\\n\\r]+', str(text))\n    parts = [p.strip() for p in parts if p.strip()]",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "clean_multiline",
        "kind": 2,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "def clean_multiline(text):\n    \"\"\"Convert multiline text to properly comma-separated clean string.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\"]:\n        return \"-\"\n    text = remove_refs(text)\n    parts = re.split(r'[\\n\\r]+', str(text))\n    parts = [p.strip() for p in parts if p.strip()]\n    return \", \".join(parts) if parts else \"-\"\ndef clean_description(desc):\n    \"\"\"Clean [1], etc. and normalize spaces.\"\"\"",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "clean_description",
        "kind": 2,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "def clean_description(desc):\n    \"\"\"Clean [1], etc. and normalize spaces.\"\"\"\n    if not desc:\n        return \"-\"\n    desc = remove_refs(desc)\n    desc = re.sub(r'\\s+', ' ', desc).strip()\n    return desc if desc else \"-\"\ndef extract_years_from_release(text):\n    \"\"\"Extract release year(s) from release date text.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\", \"-\"]:",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "extract_years_from_release",
        "kind": 2,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "def extract_years_from_release(text):\n    \"\"\"Extract release year(s) from release date text.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\", \"-\"]:\n        return \"-\"\n    text = remove_refs(text)\n    # Match 4-digit years (e.g., 2004, 2015, 2020)\n    years = re.findall(r'(19|20)\\d{2}', text)\n    # Join unique years (sorted in order of appearance)\n    if not years:\n        return \"-\"",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "get_description_fallback",
        "kind": 2,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "def get_description_fallback(url):\n    headers = {\n        \"User-Agent\": (\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n            \"Chrome/118.0.0.0 Safari/537.36\"\n        )\n    }\n    try:\n        response = requests.get(url, headers=headers, timeout=10)",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "chrome_options",
        "kind": 5,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "chrome_options = Options()\nchrome_options.add_argument(\"--headless\")\nchrome_options.add_argument(\"--no-sandbox\")\nchrome_options.add_argument(\"--disable-dev-shm-usage\")\ndriver = webdriver.Chrome(options=chrome_options)\n# ---------------------------\n# Wikipedia main list\n# ---------------------------\nbase_url = \"https://en.wikipedia.org/wiki/List_of_South_Korean_dramas\"\ndriver.get(base_url)",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "driver",
        "kind": 5,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "driver = webdriver.Chrome(options=chrome_options)\n# ---------------------------\n# Wikipedia main list\n# ---------------------------\nbase_url = \"https://en.wikipedia.org/wiki/List_of_South_Korean_dramas\"\ndriver.get(base_url)\ntime.sleep(3)\n# ---------------------------\n# Collect all drama links\n# ---------------------------",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "base_url",
        "kind": 5,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "base_url = \"https://en.wikipedia.org/wiki/List_of_South_Korean_dramas\"\ndriver.get(base_url)\ntime.sleep(3)\n# ---------------------------\n# Collect all drama links\n# ---------------------------\nprint(\"Collecting drama links...\")\ndrama_links = []\nelems = driver.find_elements(By.CLASS_NAME, \"div-col\")\nfor elem in elems:",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "drama_links",
        "kind": 5,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "drama_links = []\nelems = driver.find_elements(By.CLASS_NAME, \"div-col\")\nfor elem in elems:\n    links = elem.find_elements(By.CSS_SELECTOR, \"a[href]\")\n    for link in links:\n        href = link.get_attribute(\"href\")\n        if href and \"wiki\" in href and \"redlink\" not in href:\n            drama_links.append(href)\ndrama_links = list(set(drama_links))\nprint(f\"Found {len(drama_links)} drama links\")",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "elems",
        "kind": 5,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "elems = driver.find_elements(By.CLASS_NAME, \"div-col\")\nfor elem in elems:\n    links = elem.find_elements(By.CSS_SELECTOR, \"a[href]\")\n    for link in links:\n        href = link.get_attribute(\"href\")\n        if href and \"wiki\" in href and \"redlink\" not in href:\n            drama_links.append(href)\ndrama_links = list(set(drama_links))\nprint(f\"Found {len(drama_links)} drama links\")\n# ---------------------------",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "drama_links",
        "kind": 5,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "drama_links = list(set(drama_links))\nprint(f\"Found {len(drama_links)} drama links\")\n# ---------------------------\n# Scrape each drama page\n# ---------------------------\ntitles, alt_titles, writers, directors, casts, genres, networks, episodes, releases, release_years, posters, descriptions = [], [], [], [], [], [], [], [], [], [], [], []\nfor i, url in enumerate(drama_links):  # you can limit with [:10] for testing\n    print(f\"\\nScraping ({i+1}/{len(drama_links)}): {url}\")\n    driver.get(url)\n    time.sleep(1.5)",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "wiki_scrapper_selenium",
        "description": "wiki_scrapper_selenium",
        "peekOfCode": "df = pd.DataFrame({\n    \"Title\": titles,\n    \"Also Known As\": alt_titles,\n    \"Written By\": writers,\n    \"Director\": directors,\n    \"Cast\": casts,\n    \"Genre\": genres,\n    \"Network\": networks,\n    \"Episodes\": episodes,\n    \"Release Dates\": releases,",
        "detail": "wiki_scrapper_selenium",
        "documentation": {}
    }
]