[
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Query",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "faiss",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "faiss",
        "description": "faiss",
        "detail": "faiss",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "CrossEncoder",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "process",
        "importPath": "rapidfuzz",
        "description": "rapidfuzz",
        "isExtraImport": true,
        "detail": "rapidfuzz",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "rapidfuzz",
        "description": "rapidfuzz",
        "isExtraImport": true,
        "detail": "rapidfuzz",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "BM25Okapi",
        "importPath": "rank_bm25",
        "description": "rank_bm25",
        "isExtraImport": true,
        "detail": "rank_bm25",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "webdriver",
        "importPath": "selenium",
        "description": "selenium",
        "isExtraImport": true,
        "detail": "selenium",
        "documentation": {}
    },
    {
        "label": "By",
        "importPath": "selenium.webdriver.common.by",
        "description": "selenium.webdriver.common.by",
        "isExtraImport": true,
        "detail": "selenium.webdriver.common.by",
        "documentation": {}
    },
    {
        "label": "Options",
        "importPath": "selenium.webdriver.chrome.options",
        "description": "selenium.webdriver.chrome.options",
        "isExtraImport": true,
        "detail": "selenium.webdriver.chrome.options",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "fuzzy_match_title",
        "kind": 2,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "def fuzzy_match_title(user_input: str, threshold=70):\n    match, score, _ = process.extractOne(user_input, titles, scorer=fuzz.WRatio)\n    if score >= threshold:\n        return match, score\n    return None, score\n@lru_cache(maxsize=128)\ndef cached_encode(text: str):\n    emb = model.encode([text], convert_to_numpy=True)\n    faiss.normalize_L2(emb)\n    return emb",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "cached_encode",
        "kind": 2,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "def cached_encode(text: str):\n    emb = model.encode([text], convert_to_numpy=True)\n    faiss.normalize_L2(emb)\n    return emb\n# ======================================================\n# Hybrid Recommendation Logic\n# ======================================================\ndef recommend(title: str, top_n=5, alpha=0.7):\n    # Try to find by title\n    drama = next((m for m in metadata if m[\"Title\"].lower() == title.lower()), None)",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "recommend",
        "kind": 2,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "def recommend(title: str, top_n=5, alpha=0.7):\n    # Try to find by title\n    drama = next((m for m in metadata if m[\"Title\"].lower() == title.lower()), None)\n    if not drama:\n        match, score = fuzzy_match_title(title)\n        if match:\n            drama = next((m for m in metadata if m[\"Title\"] == match), None)\n            print(f\"Fuzzy match: '{title}' â†’ '{match}' ({score:.1f}%)\")\n        else:\n            print(f\"No close match for '{title}', treating as query text.\")",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "root",
        "kind": 2,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "def root():\n    return {\"message\": \"Hybrid Kdrama Recommendation API v3.0 is running\"}\n@app.get(\"/recommend\")\ndef get_recommendations(\n    title: str = Query(..., description=\"Kdrama title or query\"),\n    top_n: int = 5\n):\n    return recommend(title, top_n)\n# ======================================================\n# Run Server",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "get_recommendations",
        "kind": 2,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "def get_recommendations(\n    title: str = Query(..., description=\"Kdrama title or query\"),\n    top_n: int = 5\n):\n    return recommend(title, top_n)\n# ======================================================\n# Run Server\n# ======================================================\nif __name__ == \"__main__\":\n    import uvicorn",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "MODEL_NAME = \"paraphrase-multilingual-mpnet-base-v2\"\nCROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # optional reranker\nMODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\n# ======================================================\n# App Setup\n# ======================================================\napp = FastAPI(title=\"Kdrama Hybrid Recommendation API\", version=\"3.0\")\napp.add_middleware(\n    CORSMiddleware,",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "CROSS_ENCODER_MODEL",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # optional reranker\nMODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\n# ======================================================\n# App Setup\n# ======================================================\napp = FastAPI(title=\"Kdrama Hybrid Recommendation API\", version=\"3.0\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "MODEL_DIR",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "MODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\n# ======================================================\n# App Setup\n# ======================================================\napp = FastAPI(title=\"Kdrama Hybrid Recommendation API\", version=\"3.0\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "INDEX_DIR",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "INDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\n# ======================================================\n# App Setup\n# ======================================================\napp = FastAPI(title=\"Kdrama Hybrid Recommendation API\", version=\"3.0\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "app = FastAPI(title=\"Kdrama Hybrid Recommendation API\", version=\"3.0\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n# ======================================================\n# Load models and data",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "model = SentenceTransformer(MODEL_NAME, cache_folder=MODEL_DIR)\nindex = faiss.read_index(os.path.join(INDEX_DIR, \"index.faiss\"))\nwith open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"rb\") as f:\n    metadata = pickle.load(f)\ntitles = [m[\"Title\"] for m in metadata]\ncorpus = [f\"{m.get('Title', '')} {m.get('Genre', '')} {m.get('Description', '')} {m.get('Cast', '')}\" for m in metadata]\nbm25 = BM25Okapi([doc.split() for doc in corpus])\nprint(f\"Loaded {len(metadata)} dramas successfully!\")\n# Optional cross-encoder reranker (semantic reranking)\ntry:",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "index = faiss.read_index(os.path.join(INDEX_DIR, \"index.faiss\"))\nwith open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"rb\") as f:\n    metadata = pickle.load(f)\ntitles = [m[\"Title\"] for m in metadata]\ncorpus = [f\"{m.get('Title', '')} {m.get('Genre', '')} {m.get('Description', '')} {m.get('Cast', '')}\" for m in metadata]\nbm25 = BM25Okapi([doc.split() for doc in corpus])\nprint(f\"Loaded {len(metadata)} dramas successfully!\")\n# Optional cross-encoder reranker (semantic reranking)\ntry:\n    reranker = CrossEncoder(CROSS_ENCODER_MODEL)",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "titles",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "titles = [m[\"Title\"] for m in metadata]\ncorpus = [f\"{m.get('Title', '')} {m.get('Genre', '')} {m.get('Description', '')} {m.get('Cast', '')}\" for m in metadata]\nbm25 = BM25Okapi([doc.split() for doc in corpus])\nprint(f\"Loaded {len(metadata)} dramas successfully!\")\n# Optional cross-encoder reranker (semantic reranking)\ntry:\n    reranker = CrossEncoder(CROSS_ENCODER_MODEL)\n    use_reranker = True\n    print(\"Cross-encoder reranker loaded successfully.\")\nexcept Exception as e:",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "corpus",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "corpus = [f\"{m.get('Title', '')} {m.get('Genre', '')} {m.get('Description', '')} {m.get('Cast', '')}\" for m in metadata]\nbm25 = BM25Okapi([doc.split() for doc in corpus])\nprint(f\"Loaded {len(metadata)} dramas successfully!\")\n# Optional cross-encoder reranker (semantic reranking)\ntry:\n    reranker = CrossEncoder(CROSS_ENCODER_MODEL)\n    use_reranker = True\n    print(\"Cross-encoder reranker loaded successfully.\")\nexcept Exception as e:\n    reranker = None",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "bm25",
        "kind": 5,
        "importPath": "backend.app",
        "description": "backend.app",
        "peekOfCode": "bm25 = BM25Okapi([doc.split() for doc in corpus])\nprint(f\"Loaded {len(metadata)} dramas successfully!\")\n# Optional cross-encoder reranker (semantic reranking)\ntry:\n    reranker = CrossEncoder(CROSS_ENCODER_MODEL)\n    use_reranker = True\n    print(\"Cross-encoder reranker loaded successfully.\")\nexcept Exception as e:\n    reranker = None\n    use_reranker = False",
        "detail": "backend.app",
        "documentation": {}
    },
    {
        "label": "remove_refs",
        "kind": 2,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "def remove_refs(text):\n    \"\"\"Remove all reference markers like [1], [ko], [citation needed].\"\"\"\n    return re.sub(r'\\[.*?\\]', '', str(text))\ndef clean_multiline(text):\n    \"\"\"Convert multiline text to properly comma-separated clean string.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\"]:\n        return \"-\"\n    text = remove_refs(text)\n    parts = re.split(r'[\\n\\r]+', str(text))\n    parts = [p.strip() for p in parts if p.strip()]",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "clean_multiline",
        "kind": 2,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "def clean_multiline(text):\n    \"\"\"Convert multiline text to properly comma-separated clean string.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\"]:\n        return \"-\"\n    text = remove_refs(text)\n    parts = re.split(r'[\\n\\r]+', str(text))\n    parts = [p.strip() for p in parts if p.strip()]\n    return \", \".join(parts) if parts else \"-\"\ndef clean_description(desc):\n    \"\"\"Clean [1], etc. and normalize spaces.\"\"\"",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "clean_description",
        "kind": 2,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "def clean_description(desc):\n    \"\"\"Clean [1], etc. and normalize spaces.\"\"\"\n    if not desc:\n        return \"-\"\n    desc = remove_refs(desc)\n    desc = re.sub(r'\\s+', ' ', desc).strip()\n    return desc if desc else \"-\"\ndef extract_years_from_release(text):\n    \"\"\"Extract release year(s) from release date text.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\", \"-\"]:",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "extract_years_from_release",
        "kind": 2,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "def extract_years_from_release(text):\n    \"\"\"Extract release year(s) from release date text.\"\"\"\n    if not text or str(text).strip().lower() in [\"n/a\", \"na\", \"-\"]:\n        return \"-\"\n    text = remove_refs(text)\n    # Match 4-digit years (e.g., 2004, 2015, 2020)\n    years = re.findall(r'(19|20)\\d{2}', text)\n    # Join unique years (sorted in order of appearance)\n    if not years:\n        return \"-\"",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "get_description_fallback",
        "kind": 2,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "def get_description_fallback(url):\n    headers = {\n        \"User-Agent\": (\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n            \"Chrome/118.0.0.0 Safari/537.36\"\n        )\n    }\n    try:\n        response = requests.get(url, headers=headers, timeout=10)",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "chrome_options",
        "kind": 5,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "chrome_options = Options()\nchrome_options.add_argument(\"--headless\")\nchrome_options.add_argument(\"--no-sandbox\")\nchrome_options.add_argument(\"--disable-dev-shm-usage\")\ndriver = webdriver.Chrome(options=chrome_options)\n# ---------------------------\n# Wikipedia main list\n# ---------------------------\nbase_url = \"https://en.wikipedia.org/wiki/List_of_South_Korean_dramas\"\ndriver.get(base_url)",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "driver",
        "kind": 5,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "driver = webdriver.Chrome(options=chrome_options)\n# ---------------------------\n# Wikipedia main list\n# ---------------------------\nbase_url = \"https://en.wikipedia.org/wiki/List_of_South_Korean_dramas\"\ndriver.get(base_url)\ntime.sleep(3)\n# ---------------------------\n# Collect all drama links\n# ---------------------------",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "base_url",
        "kind": 5,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "base_url = \"https://en.wikipedia.org/wiki/List_of_South_Korean_dramas\"\ndriver.get(base_url)\ntime.sleep(3)\n# ---------------------------\n# Collect all drama links\n# ---------------------------\nprint(\"Collecting drama links...\")\ndrama_links = []\nelems = driver.find_elements(By.CLASS_NAME, \"div-col\")\nfor elem in elems:",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "drama_links",
        "kind": 5,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "drama_links = []\nelems = driver.find_elements(By.CLASS_NAME, \"div-col\")\nfor elem in elems:\n    links = elem.find_elements(By.CSS_SELECTOR, \"a[href]\")\n    for link in links:\n        href = link.get_attribute(\"href\")\n        if href and \"wiki\" in href and \"redlink\" not in href:\n            drama_links.append(href)\ndrama_links = list(set(drama_links))\nprint(f\"Found {len(drama_links)} drama links\")",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "elems",
        "kind": 5,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "elems = driver.find_elements(By.CLASS_NAME, \"div-col\")\nfor elem in elems:\n    links = elem.find_elements(By.CSS_SELECTOR, \"a[href]\")\n    for link in links:\n        href = link.get_attribute(\"href\")\n        if href and \"wiki\" in href and \"redlink\" not in href:\n            drama_links.append(href)\ndrama_links = list(set(drama_links))\nprint(f\"Found {len(drama_links)} drama links\")\n# ---------------------------",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "drama_links",
        "kind": 5,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "drama_links = list(set(drama_links))\nprint(f\"Found {len(drama_links)} drama links\")\n# ---------------------------\n# Scrape each drama page\n# ---------------------------\ntitles, alt_titles, writers, directors, casts, genres, networks, episodes, releases, release_years, posters, descriptions = [], [], [], [], [], [], [], [], [], [], [], []\nfor i, url in enumerate(drama_links[:100]):  # you can limit with [:10] for testing\n    print(f\"\\nScraping ({i+1}/{len(drama_links)}): {url}\")\n    driver.get(url)\n    time.sleep(1.5)",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "data_scrapper.scrapper",
        "description": "data_scrapper.scrapper",
        "peekOfCode": "df = pd.DataFrame({\n    \"Title\": titles,\n    \"Also Known As\": alt_titles,\n    \"Written By\": writers,\n    \"Director\": directors,\n    \"Cast\": casts,\n    \"Genre\": genres,\n    \"Network\": networks,\n    \"Episodes\": episodes,\n    \"Release Dates\": releases,",
        "detail": "data_scrapper.scrapper",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "kind": 2,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "def clean_text(text):\n    return \" \".join(str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").split())\nfor col in df.columns:\n    df[col] = df[col].astype(str).apply(clean_text)\n# Create unified text field for embeddings\ntext_features = [\"Title\", \"Genre\", \"Description\", \"Cast\", \"Director\", \"Also Known As\", \"Network\", \"Release Years\"]\ndf[\"content\"] = df[[col for col in text_features if col in df.columns]].agg(\" \".join, axis=1)\n# ======================================================\n# 3. Load SentenceTransformer Model\n# ======================================================",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "DATA_PATH = r\"D:\\Projects\\Kdrama-recomendation\\data_scrapper\\kdrama_dataset_detailed_v7.csv\"\nMODEL_NAME = \"paraphrase-multilingual-mpnet-base-v2\"\nMODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(INDEX_DIR, exist_ok=True)\n# ======================================================\n# 2. Load and Prepare Dataset\n# ======================================================\nprint(\"Loading dataset...\")",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "MODEL_NAME = \"paraphrase-multilingual-mpnet-base-v2\"\nMODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(INDEX_DIR, exist_ok=True)\n# ======================================================\n# 2. Load and Prepare Dataset\n# ======================================================\nprint(\"Loading dataset...\")\ndf = pd.read_csv(DATA_PATH, encoding=\"utf-8\", low_memory=False)",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "MODEL_DIR",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "MODEL_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\models\"\nINDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(INDEX_DIR, exist_ok=True)\n# ======================================================\n# 2. Load and Prepare Dataset\n# ======================================================\nprint(\"Loading dataset...\")\ndf = pd.read_csv(DATA_PATH, encoding=\"utf-8\", low_memory=False)\ndf.fillna(\"\", inplace=True)",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "INDEX_DIR",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "INDEX_DIR = r\"D:\\Projects\\Kdrama-recommendation\\model_training\\faiss_index\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(INDEX_DIR, exist_ok=True)\n# ======================================================\n# 2. Load and Prepare Dataset\n# ======================================================\nprint(\"Loading dataset...\")\ndf = pd.read_csv(DATA_PATH, encoding=\"utf-8\", low_memory=False)\ndf.fillna(\"\", inplace=True)\n# Ensure required columns exist",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "df = pd.read_csv(DATA_PATH, encoding=\"utf-8\", low_memory=False)\ndf.fillna(\"\", inplace=True)\n# Ensure required columns exist\nfor col in [\"Title\", \"Genre\", \"Description\", \"Cast\"]:\n    if col not in df.columns:\n        df[col] = \"\"\n# Normalize text fields\ndef clean_text(text):\n    return \" \".join(str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").split())\nfor col in df.columns:",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "text_features",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "text_features = [\"Title\", \"Genre\", \"Description\", \"Cast\", \"Director\", \"Also Known As\", \"Network\", \"Release Years\"]\ndf[\"content\"] = df[[col for col in text_features if col in df.columns]].agg(\" \".join, axis=1)\n# ======================================================\n# 3. Load SentenceTransformer Model\n# ======================================================\nprint(\"Loading SentenceTransformer model...\")\nmodel = SentenceTransformer(MODEL_NAME, cache_folder=MODEL_DIR)\nprint(\"Model loaded successfully!\")\n# ======================================================\n# 4. Generate Embeddings",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "df[\"content\"]",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "df[\"content\"] = df[[col for col in text_features if col in df.columns]].agg(\" \".join, axis=1)\n# ======================================================\n# 3. Load SentenceTransformer Model\n# ======================================================\nprint(\"Loading SentenceTransformer model...\")\nmodel = SentenceTransformer(MODEL_NAME, cache_folder=MODEL_DIR)\nprint(\"Model loaded successfully!\")\n# ======================================================\n# 4. Generate Embeddings\n# ======================================================",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "model = SentenceTransformer(MODEL_NAME, cache_folder=MODEL_DIR)\nprint(\"Model loaded successfully!\")\n# ======================================================\n# 4. Generate Embeddings\n# ======================================================\nprint(\"Generating embeddings (this may take a few minutes)...\")\nembeddings = model.encode(\n    df[\"content\"].tolist(),\n    show_progress_bar=True,\n    convert_to_numpy=True,",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "embeddings = model.encode(\n    df[\"content\"].tolist(),\n    show_progress_bar=True,\n    convert_to_numpy=True,\n    batch_size=32\n)\n# ======================================================\n# 5. Build FAISS Index\n# ======================================================\ndim = embeddings.shape[1]",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "dim",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "dim = embeddings.shape[1]\nindex = faiss.IndexFlatIP(dim)  # Inner product for cosine similarity\nfaiss.normalize_L2(embeddings)\nindex.add(embeddings)\nprint(f\"FAISS index built successfully with {index.ntotal} items.\")\n# ======================================================\n# 6. Save Index and Metadata\n# ======================================================\nfaiss.write_index(index, os.path.join(INDEX_DIR, \"index.faiss\"))\n# Save relevant metadata (keep it clean for inference)",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "index = faiss.IndexFlatIP(dim)  # Inner product for cosine similarity\nfaiss.normalize_L2(embeddings)\nindex.add(embeddings)\nprint(f\"FAISS index built successfully with {index.ntotal} items.\")\n# ======================================================\n# 6. Save Index and Metadata\n# ======================================================\nfaiss.write_index(index, os.path.join(INDEX_DIR, \"index.faiss\"))\n# Save relevant metadata (keep it clean for inference)\nmeta_cols = [\"Title\", \"Genre\", \"Description\", \"Cast\", \"Director\", \"Network\", \"Release Years\", \"Also Known As\"]",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "meta_cols",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "meta_cols = [\"Title\", \"Genre\", \"Description\", \"Cast\", \"Director\", \"Network\", \"Release Years\", \"Also Known As\"]\nmeta_cols = [c for c in meta_cols if c in df.columns]\nmetadata = df[meta_cols].to_dict(orient=\"records\")\nwith open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"wb\") as f:\n    pickle.dump(metadata, f)\nprint(f\"Index and metadata saved in: {INDEX_DIR}\")\nprint(\"All done! Your FAISS index is ready for recommendations.\")",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "meta_cols",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "meta_cols = [c for c in meta_cols if c in df.columns]\nmetadata = df[meta_cols].to_dict(orient=\"records\")\nwith open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"wb\") as f:\n    pickle.dump(metadata, f)\nprint(f\"Index and metadata saved in: {INDEX_DIR}\")\nprint(\"All done! Your FAISS index is ready for recommendations.\")",
        "detail": "model_traning.build_index",
        "documentation": {}
    },
    {
        "label": "metadata",
        "kind": 5,
        "importPath": "model_traning.build_index",
        "description": "model_traning.build_index",
        "peekOfCode": "metadata = df[meta_cols].to_dict(orient=\"records\")\nwith open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"wb\") as f:\n    pickle.dump(metadata, f)\nprint(f\"Index and metadata saved in: {INDEX_DIR}\")\nprint(\"All done! Your FAISS index is ready for recommendations.\")",
        "detail": "model_traning.build_index",
        "documentation": {}
    }
]